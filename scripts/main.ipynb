{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea68ff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing librairies\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from ultralytics import YOLO\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "74fe4b0a",
   "metadata": {},
   "source": [
    "Converting XML to YOLO Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5ec282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "xml_folder = '../data/annotations'       \n",
    "image_folder = '../data/images'          \n",
    "train_output_folder = '../data/train/labels' \n",
    "train_folder = '../data/train/images'  \n",
    "val_output_folder = '../data/val/labels'  \n",
    "val_folder = '../data/val/images'       \n",
    "\n",
    "# Creating output folders if they don't exist\n",
    "Path(train_output_folder).mkdir(parents=True, exist_ok=True)\n",
    "Path(train_folder).mkdir(parents=True, exist_ok=True)\n",
    "Path(val_output_folder).mkdir(parents=True, exist_ok=True)\n",
    "Path(val_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# We will go through all the XML files\n",
    "xml_files = [f for f in os.listdir(xml_folder) if f.endswith('.xml')]\n",
    "\n",
    "# Splitting XML files into train and validation sets (80% train, 20% validation)\n",
    "train_files = xml_files[:int(0.8 * len(xml_files))]  # 80% for training\n",
    "val_files = xml_files[int(0.8 * len(xml_files)):]    # 20% for validation\n",
    "\n",
    "\n",
    "def xml_to_yolo(xml_file, image_name, output_folder):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Opening corresponding .txt file to save YOLO annotations\n",
    "    txt_file_path = os.path.join(output_folder, image_name.replace('.png', '.txt'))\n",
    "    with open(txt_file_path, 'w') as f:\n",
    "        for obj in root.findall('object'):\n",
    "            # Get the object class name\n",
    "            class_name = obj.find('name').text\n",
    "\n",
    "            class_id = 0  # 0 is used for single class\n",
    "            \n",
    "            # Getting bounding box coordinates\n",
    "            bndbox = obj.find('bndbox')\n",
    "            xmin = int(bndbox.find('xmin').text)\n",
    "            ymin = int(bndbox.find('ymin').text)\n",
    "            xmax = int(bndbox.find('xmax').text)\n",
    "            ymax = int(bndbox.find('ymax').text)\n",
    "            \n",
    "            # Convertting to YOLO format\n",
    "            image_path = os.path.join(image_folder, image_name)\n",
    "            image_width, image_height = 640, 640 \n",
    "            \n",
    "            # Calculating YOLO format coordinates\n",
    "            x_center = (xmin + xmax) / 2 / image_width\n",
    "            y_center = (ymin + ymax) / 2 / image_height\n",
    "            width = (xmax - xmin) / image_width\n",
    "            height = (ymax - ymin) / image_height\n",
    "            \n",
    "            # Writing to the YOLO annotation file\n",
    "            f.write(f\"{class_id} {x_center} {y_center} {width} {height}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6b8aec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "Processing validation data...\n",
      "Dataset preparation complete.\n"
     ]
    }
   ],
   "source": [
    "# Function to process images and annotations\n",
    "def process_images(xml_files, output_folder, image_folder, label_folder):\n",
    "    for xml_file in xml_files:\n",
    "        image_name = xml_file.replace('.xml', '.png')  # Assuming image extension is .png\n",
    "        image_path = os.path.join(image_folder, image_name)\n",
    "\n",
    "        if os.path.exists(image_path):\n",
    "            # Path to the XML file\n",
    "            xml_file_path = os.path.join(xml_folder, xml_file)\n",
    "            \n",
    "            # Converting XML to YOLO format\n",
    "            xml_to_yolo(xml_file_path, image_name, label_folder)\n",
    "            \n",
    "            # Copying image to the appropriate folder (train or validation)\n",
    "            shutil.copy(image_path, output_folder)\n",
    "        else:\n",
    "            print(f\"Warning: Image {image_name} not found, skipping.\")\n",
    "\n",
    "# Processing training data\n",
    "print(\"Processing training data...\")\n",
    "process_images(train_files, train_folder, image_folder, train_output_folder)\n",
    "\n",
    "# Processing validation data\n",
    "print(\"Processing validation data...\")\n",
    "process_images(val_files, val_folder, image_folder, val_output_folder)\n",
    "\n",
    "print(\"Dataset preparation complete.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e665cded",
   "metadata": {},
   "source": [
    "Creating the data.yaml File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98533f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.yaml file created at ../data.yaml.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the path for the data.yaml file in the project root\n",
    "data_yaml_path = '../data.yaml'\n",
    "\n",
    "# Define the content of the data.yaml file\n",
    "data_yaml = \"\"\"\n",
    "train: ALPR/data/images  # Path to training images\n",
    "val: ALPR/data/images      # Path to validation images\n",
    "\n",
    "nc: 1                       # Number of classes (in your case, 1 for license plates or other)\n",
    "\n",
    "names: ['license_plate']     # Name of your class (customize as needed)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Save the YAML file to the project root directory\n",
    "with open(data_yaml_path, 'w') as f:\n",
    "    f.write(data_yaml)\n",
    "\n",
    "print(f\"data.yaml file created at {data_yaml_path}.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "48c13aea",
   "metadata": {},
   "source": [
    "Training YOLO on the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5511ba67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.32  Python-3.11.7 torch-2.3.0+cpu CPU (12th Gen Intel Core(TM) i7-1265U)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=../data.yaml, epochs=5, time=None, patience=100, batch=32, imgsz=416, save=True, save_period=-1, cache=ram, device=cpu, workers=4, project=None, name=train25, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train25\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
      "Model summary: 225 layers, 11,135,987 parameters, 11,135,971 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train25', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Eyram\\Documents\\GitHub\\ALPR\\data\\labels.cache... 26 images, 401 backgrounds, 0 corrupt: 100%|██████████| 427/427 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB RAM): 100%|██████████| 427/427 [00:00<00:00, 591.47it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Eyram\\Documents\\GitHub\\ALPR\\data\\labels.cache... 26 images, 401 backgrounds, 0 corrupt: 100%|██████████| 427/427 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB RAM): 100%|██████████| 427/427 [00:00<00:00, 591.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train25\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 416 train, 416 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train25\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G      3.618      97.86       2.46          2        416: 100%|██████████| 14/14 [02:16<00:00,  9.74s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:48<00:00,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        427         26    0.00582     0.0385   0.000243   0.000115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G       2.89      20.11      1.735          0        416: 100%|██████████| 14/14 [02:44<00:00, 11.75s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:49<00:00,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        427         26   0.000175     0.0385   2.85e-05   1.14e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G       3.96      8.225      2.247          4        416: 100%|██████████| 14/14 [02:50<00:00, 12.18s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:53<00:00,  7.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        427         26   5.73e-05      0.269   4.28e-05   6.88e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G      3.763      6.338      2.268          1        416: 100%|██████████| 14/14 [02:55<00:00, 12.54s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:52<00:00,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        427         26    0.00012      0.115   7.29e-05   1.89e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Initializing the YOLOv8 model\n",
    "model = YOLO('yolov8s.pt')  # Loading the YOLOv8 model\n",
    "\n",
    "# Training with YOLOv8\n",
    "model.train(\n",
    "    data=data_yaml_path,       # Path to the YAML file\n",
    "    imgsz=416,                 # Image size\n",
    "    batch=32,                  # Bbatch size\n",
    "    epochs=5,                  # Number of epochs. I am setting it to 5 so it does not take forever to train\n",
    "    cache='ram',               # We will use RAM caching for faster data loading\n",
    "    amp=True,                  # Automatic mixed precision for faster training\n",
    "    workers=4,                 # Number of workers for data loading\n",
    "    device='cpu'               # Because my computer uses CPU and not GPU\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462440d8",
   "metadata": {},
   "source": [
    "Using the Trained YOLO Model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08e6aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Load the trained YOLO model\n",
    "model = YOLO(\"runs/detect/train17/weights/best.pt\")\n",
    "\n",
    "# Let's set the paths\n",
    "test_folder = \"../data/test\"  # Folder containing test images\n",
    "save_folder = \"runs/detect/test_results\"  # Folder to save results\n",
    "os.makedirs(save_folder, exist_ok=True)  # Creating folder if it doesn't exist\n",
    "\n",
    "# Confidence and IoU thresholds\n",
    "CONF_THRESHOLD = 0.5  # Minimum confidence score\n",
    "IOU_THRESHOLD = 0.5   # IoU threshold for Non-Maximum Suppression (NMS)\n",
    "\n",
    "# Now we will process each .png image in the test folder\n",
    "for image_name in os.listdir(test_folder):\n",
    "    if image_name.lower().endswith(\".png\"):\n",
    "        image_path = os.path.join(test_folder, image_name)\n",
    "        print(f\"Processing: {image_name}\")\n",
    "        \n",
    "        # We will now perform inference\n",
    "        results = model(image_path, conf=CONF_THRESHOLD, iou=IOU_THRESHOLD)\n",
    "\n",
    "        # Getting the detections (bounding boxes)\n",
    "        detections = results[0].boxes\n",
    "        if detections:\n",
    "            print(f\"Detections for {image_name}:\")\n",
    "            for box in detections:\n",
    "                print(box.xywh)  # Printing bounding box details (x, y, w, h)\n",
    "            \n",
    "            # Visualizing results on the image\n",
    "            annotated_image = results[0].plot()  # Annotating the image\n",
    "            \n",
    "            # Saveingthe annotated image with \"detect\" in the filename\n",
    "            new_name = f\"detect_{image_name}\"\n",
    "            save_path = os.path.join(save_folder, new_name)\n",
    "            cv2.imwrite(save_path, annotated_image)\n",
    "            print(f\"Saved results to {save_path}\")\n",
    "            \n",
    "            # Displaying the annotated image\n",
    "            cv2.imshow(f\"Detections - {new_name}\", annotated_image)\n",
    "            cv2.waitKey(0)  # Press any key to close the image\n",
    "            cv2.destroyAllWindows()\n",
    "        else:\n",
    "            print(f\"No detections for {image_name}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a51761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the metrics\n",
    "metrics = model.val(data='../data.yaml')\n",
    "print(metrics)  # Includes mAP, precision, recall\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
